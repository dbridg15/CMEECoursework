
\section{How far can machine listening help with acoustic bird surveying?}

\textbf{Speaker:} Dr Dan Stowell (\textit{Queen Mary, University of London})\\
\textbf{Date:} \hspace{.53cm} 24\textsuperscript{th} May 2018
\vspace{.5cm}

Birdsong recordings can carry a lot of information: which species are present, how many birds are present, how the birds are interacting. Machine learning algorithms could be used to answer questions similar to this on a much larger scale, both on contemporary recordings and archives. Traditional measures in birdsong include: frequency range, syllable count/duration and bout duration. Classification of birds is complicated by the fact that many species have 'regional dialects'. A common method for machine learning in birdsong is 'Feature Matching' where slices of audio (just a few milliseconds) are matched with templates. Similar to image recognition neural networks can be 'trained' on tagged audio clips.

Dan and his colleagues have developed an app 'Warblr' based on the neural network they have produced. The app can take a short recording of birdsong and return a ranked list of birds present with a percentage confidence. They combine this with geographical data from the ornithological society to produce better predictions. A number of insights have come from this endeavour including confusing results from around Camden (where ZSL is) and many recordings which do not have any birds in but still the app attempts to classify.

Beyond classification, detection is the next big challenge - this would be the ability to detect where in a recording a species is present, as well as what that species is. Using data from the Warblr app and other crowdsourced data they now have a fairly substantial 'training' dataset which can be used in machine learning. They have set up a 'Bird Audio Detection Challenge' one ran in 2016/17 and one is open now. This is an efficient way of pooling expertise from a wide variety of sources.

